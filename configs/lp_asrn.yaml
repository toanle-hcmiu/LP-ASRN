# LP-ASRN Configuration
# Layout-Aware and Character-Driven Super-Resolution Network
# Enhanced with LCOFL-EC, DCNv4, Multi-Scale Character Attention, and OCR-Driven Curriculum

model:
  # Number of RRDB-EA blocks in the deep feature extractor
  # Balanced: 12 blocks for good capacity without being excessive (original: 24, papers: 16)
  num_rrdb_blocks: 12

  # Number of feature channels
  # Reduced from 128 to 64 for lightweight model (4x parameter reduction)
  num_filters: 64

  # Upscaling factor (2 for 2x, 4 for 4x)
  # Paper 2 uses 2x which achieved 49.8% recognition rate
  upscale_factor: 2

  # Use deformable convolutions in attention modules
  use_deformable: true

  # Phase 2: DCNv4 Configuration
  use_dcnv4: true             # Use DCNv4 instead of DCNv3 (falls back to DCNv3 if unavailable)
  dcnv4_groups: 4             # Number of groups for DCNv4

  # Phase 3: Multi-Scale Character Attention
  # DISABLED for lightweight model - saves computation and params
  use_character_attention: false  # Disabled for speed (not needed for fixed-format plates)
  msca_scales: [1.0, 0.5, 0.25]  # Scales for multi-scale processing
  msca_num_prototypes: 36          # Number of character prototypes

data:
  # Scenarios to include in training
  scenarios:
    - "Scenario-A"  # Light degradation
    - "Scenario-B"  # Heavy degradation

  # Layouts to include
  layouts:
    - "Brazilian"   # LLLNNNN pattern
    - "Mercosur"    # LLLNLNN pattern

  # Target size for LR images (height, width)
  # Increased from 17x31 to 34x62 (2x) for better OCR - each character now ~10-12 pixels instead of ~5-6
  lr_size: [34, 62]

  # Batch size for training (64 fits comfortably with lightweight model)
  batch_size: 64

  # Number of data loading workers (reduced for DDP stability)
  num_workers: 4

  # Validation split
  val_split: 0.05

  # Enable OCR-specific augmentation during pretraining
  # Set to true when running stage0 (OCR pretraining) for better robustness
  ocr_pretrain_augmentation: true

  # Aspect ratio augmentation: randomly varies crop aspect ratio during training
  # to match test-time image distribution. Test images have H/W ratios of 0.29-0.40
  # while training crops resized to lr_size have ratio 0.55. This augmentation
  # pads images before resize to simulate the test distribution, teaching the
  # generator+OCR to handle varied aspect ratios.
  aspect_ratio_augment: true
  test_aspect_range: [0.29, 0.40]  # Min/max H/W ratios observed in test-public

training:
  # Number of training epochs
  epochs: 100

  # Learning rate
  lr: 0.0001

  # Validation interval (validate every N epochs to save time)
  val_interval: 5  # Reduced validation frequency for faster training

  # Beam width for validation (higher = more accurate but slower)
  val_beam_width: 5

  # Adam optimizer betas
  beta1: 0.9
  beta2: 0.999

  # Gradient clipping for stability (prevents exploding gradients)
  gradient_clip: 1.0

  # StepLR learning rate schedule
  # Reduces LR by this factor every lr_step_size epochs
  # Paper 2 uses this to prevent oscillations
  lr_scheduler: "StepLR"
  lr_step_size: 10
  lr_gamma: 0.95

  # Early stopping patience (from Paper 1)
  # Stops after 30 epochs without improvement (increased for 98% target)
  early_stop_patience: 30

  # Validation settings
  val_interval: 2         # Validate every N epochs
  val_beam_width: 3       # Beam width for OCR during validation (3 = good balance of speed/accuracy)

  # Monitor metric for early stopping
  # "recognition_rate" monitors OCR accuracy (Paper 2 innovation)
  # "loss" monitors total loss
  monitor_metric: "recognition_rate"

  # Output directory for all training outputs (checkpoints, logs, etc.)
  # Auto-formatted with timestamp at runtime: outputs/run_YYYYMMDD_HHMMSS
  save_dir: "outputs/run_%Y%m%d_%H%M%S"

loss:
  # Weight for LCOFL loss (character-driven loss)
  lambda_lcofl: 1.0

  # Weight for layout penalty
  lambda_layout: 0.5

  # Weight for SSIM loss (from Paper 2)
  lambda_ssim: 0.2

  # Weight for VGG perceptual loss (sharper textures)
  # Active during LCOFL and finetune stages
  lambda_perceptual: 0.1

  # Phase 1: Embedding Consistency Loss (LCOFL-EC)
  # DISABLED for lightweight model - saves ResNet-18 backbone (~11M params)
  lambda_embed: 0.0           # Disabled for lightweight model
  embed_target_weight: 0.0    # Disabled for lightweight model
  embed_warmup_epochs: 50     # Epochs for warm-up
  embedding_dim: 128          # Embedding dimension
  embed_margin: 2.0           # Contrastive loss margin
  use_lightweight_embedder: false  # Use lightweight embedder instead of ResNet-18

  # Confusion weight increment (added to confused character weights)
  alpha: 0.1

  # Layout penalty value (for each digit/letter mismatch)
  beta: 1.0

ocr:
  # OCR model type
  model_type: "simplecrnn"

  # Use external pretrained model from HuggingFace (true) or SimpleCRNN (false)
  # NOTE: External model requires teacher forcing for training which is not implemented.
  # Use SimpleCRNN (false) for training.
  use_pretrained: false

  # Pre-trained model path (HuggingFace model ID or local path)
  pretrained_path: "baudm/parseq-base"

  # Fine-tune OCR on HR images before SR training
  finetune_first: true

  # Path to fine-tuned model (after fine-tuning)
  finetuned_path: "checkpoints/ocr/best.pth"

  # Keep OCR frozen during SR training (critical for stability)
  freeze_ocr: true

  # Maximum sequence length for OCR
  max_length: 7

  # Character vocabulary
  vocab: "0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ"

  # Label smoothing for loss (CTT for SimpleCRNN)
  # 0.0 = disabled, 0.1 = recommended, 0.2 = strong
  label_smoothing: 0.1  # Mild smoothing for better generalization

  # Dropout rate for RNN layer in SimpleCRNN
  # 0.0 = disabled, 0.3 = recommended, 0.5 = strong
  rnn_dropout: 0.3  # Balanced dropout for OCR pretraining

  # OCR model size configuration (lightweight settings)
  backbone_channels: 256      # Backbone final channels (256=lightweight, 384=full)
  lstm_hidden_size: 256       # LSTM hidden size (256=lightweight, 384=full)
  lstm_num_layers: 1          # LSTM layers (1=lightweight, 2=full)

# Progressive Training Configuration
progressive_training:
  enabled: true

  # Stage 0: OCR Pretraining
  # Purpose: Train OCR model on license plate data before SR training
  # This ensures OCR can provide meaningful guidance to the generator
  stage0:
    name: "pretrain"
    epochs: 50  # Reduced - model plateaus around epoch 50
    lr: 0.0005   # Reduced from 0.001 to prevent loss spikes
    loss_components: ["ocr"]
    freeze_ocr: false
    update_confusion: false

  # Stage 1: Warm-up (L1 loss only)
  # Purpose: Stabilize network before introducing complex losses
  stage1:
    name: "warmup"
    epochs: 30    # Reduced - lightweight model converges faster
    lr: 0.0001
    loss_components: ["l1"]
    freeze_ocr: true
    update_confusion: false

  # Stage 2: LCOFL Training (Character-driven loss)
  # Purpose: Optimize for character recognition with frozen OCR
  stage2:
    name: "lcofl"
    epochs: 200   # Reduced - smaller model needs less training
    lr: 0.0002   # Increased for faster convergence
    loss_components: ["l1", "lcofl"]
    freeze_ocr: true
    update_confusion: true

  # Stage 3: Fine-tuning (Joint optimization)
  # Purpose: Refine with unfrozen OCR for co-adaptation
  stage3:
    name: "finetune"
    epochs: 100   # Reduced for lightweight model
    lr: 0.00001   # Lower LR for joint training
    loss_components: ["l1", "lcofl"]
    freeze_ocr: false
    update_confusion: true

  # Phase 4: Hard Example Mining Stage (OCR-Driven Curriculum)
  # Purpose: Focus training on samples that OCR struggles with
  stage4:
    name: "hard_mining"
    epochs: 50    # Additional epochs focusing on hard examples
    lr: 0.000005  # Lower LR for fine-tuning on hard examples
    loss_components: ["l1", "lcofl"]  # No embedding loss for lightweight
    freeze_ocr: true
    update_confusion: true
    hard_mining:
      difficulty_alpha: 2.0      # Exponent for difficulty weighting
      reweight_interval: 5       # Re-weight samples every N epochs
      min_samples_seen: 100      # Min samples before weighting

# TensorBoard Configuration
tensorboard:
  enabled: true
  # Logs are stored in <save_dir>/logs subdirectory
  log_dir: "logs"
  port: 6007

  # Logging intervals
  log_interval: 5      # Log scalars every N steps
  log_images_every: 5   # Log images every N epochs
  log_histograms_every: 5  # Log histograms every N epochs

  # Maximum images to log per batch
  max_images: 16

# Evaluation settings
evaluation:
  # Batch size for evaluation (reduced from 32 to 16 due to larger input images)
  batch_size: 16

  # Metrics to compute
  metrics:
    - "psnr"        # Peak Signal-to-Noise Ratio
    - "ssim"        # Structural Similarity Index
    - "char_acc"    # Character-level accuracy
    - "word_acc"    # Word-level (full plate) accuracy

  # Number of visualization samples
  num_vis_samples: 10

  # Visualization save directory
  vis_dir: "results/visualizations"
