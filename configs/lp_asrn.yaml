model:
  upscale_factor: 2
  use_character_attention: false
  msca_scales:
  - 1.0
  - 0.5
  - 0.25
  msca_num_prototypes: 36
  num_features: 64
  num_blocks: 12
  num_layers_per_block: 3
  use_enhanced_attention: true
  use_deformable: true
  use_pyramid_attention: false
data:
  scenarios:
  - Scenario-A
  - Scenario-B
  layouts:
  - Brazilian
  - Mercosur
  lr_size:
  - 34
  - 62
  batch_size: 64
  num_workers: 4
  val_split: 0.05
  ocr_pretrain_augmentation: true
  aspect_ratio_augment: true
  test_aspect_range:
  - 0.25
  - 0.45
  # Test-resolution augmentation to match test-public distribution
  # Test images are ~17x49 (3.7x smaller) than training images (~30x87)
  test_resolution_augment: true
  test_resolution_prob: 0.7  # 70% of training data uses test-size inputs
  # JPEG compression augmentation (test images are JPGs, training are PNGs)
  jpeg_augment: true
  jpeg_quality_range:
  - 60
  - 95
  # No-crop probability: skip corner cropping to simulate test conditions
  # (test images have no corner annotations â€” plate boundaries unknown)
  no_crop_prob: 0.3
training:
  epochs: 100
  lr: 0.0001
  val_interval: 2
  val_beam_width: 3
  beta1: 0.9
  beta2: 0.999
  gradient_clip: 1.0
  lr_scheduler: StepLR
  lr_step_size: 20  # Increased from 10 for slower decay
  lr_gamma: 0.98  # Increased from 0.95 for gentler decay
  early_stop_patience: 30
  monitor_metric: recognition_rate
  save_dir: outputs/run_%Y%m%d_%H%M%S
  # Test-like validation: simulate test conditions during training
  # Provides realistic accuracy estimate vs misleading training-distribution val
  test_like_val: true
  test_like_val_fraction: 0.1  # Use 10% of val set for test-like evaluation
loss:
  lambda_lcofl: 1.5  # Increased for better character accuracy
  lambda_ocr: 0.1
  lambda_layout: 0.5
  lambda_ssim: 0.2
  lambda_perceptual: 0.1
  lambda_gradient: 0.05
  lambda_frequency: 0.05
  lambda_edge: 0.05
  lambda_gan: 0.01
  lambda_feature_matching: 0.1
  lambda_embed: 0.0
  embed_target_weight: 0.0
  embed_warmup_epochs: 50
  embedding_dim: 128
  embed_margin: 2.0
  use_lightweight_embedder: false
  alpha: 0.1
  beta: 1.0
  # GAN settings
  gan_enabled: false
  gan_type: hinge  # hinge or relativistic
  disc_num_filters: 64
  disc_num_layers: 3
  disc_update_interval: 1
  disc_start_epoch: 0
ocr:
  model_type: parseq
  pretrained_path: baudm/parseq-base
  finetune_first: true
  finetuned_path: checkpoints/ocr/best.pth
  freeze_ocr: true
  max_length: 7
  vocab: 0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ
progressive_training:
  enabled: true
  stage0:
    name: pretrain
    epochs: 50
    lr: 0.0005
    loss_components:
    - ocr
    freeze_ocr: false
    update_confusion: false
    aspect_ratio_range:
    - 0.25
    - 0.65
  stage1:
    name: warmup
    epochs: 80
    lr: 0.0001
    loss_components:
    - l1
    freeze_ocr: true
    update_confusion: false
    aspect_ratio_range:
    - 0.25
    - 0.6
  stage2:
    name: lcofl
    epochs: 200
    lr: 0.0001
    loss_components:
    - l1
    - lcofl
    freeze_ocr: true
    update_confusion: true
    aspect_ratio_range:
    - 0.2
    - 0.55
  stage3:
    name: finetune
    epochs: 200  # Increased from 100 for better convergence
    lr: 1.0e-05
    loss_components:
    - l1
    - lcofl
    - ssim
    - gradient
    - frequency
    - edge
    freeze_ocr: true
    update_confusion: true
    aspect_ratio_range:
    - 0.25
    - 0.45  # Narrowed to match actual test distribution (0.29-0.40)
  stage4:
    name: hard_mining
    epochs: 50
    lr: 5.0e-06
    loss_components:
    - l1
    - lcofl
    freeze_ocr: true
    update_confusion: true
    aspect_ratio_range:
    - 0.2
    - 0.45
    hard_mining:
      difficulty_alpha: 2.0
      reweight_interval: 5
      min_samples_seen: 100
tensorboard:
  enabled: true
  log_dir: logs
  port: 6007
  log_interval: 5
  log_images_every: 5
  log_histograms_every: 5
  max_images: 16
evaluation:
  batch_size: 64
  metrics:
  - psnr
  - ssim
  - char_acc
  - word_acc
  num_vis_samples: 10
  vis_dir: results/visualizations
