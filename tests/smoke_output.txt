============================================================
LP-ASRN SMOKE TEST
Python: 3.13.7 (tags/v3.13.7:bcee1c3, Aug 14 2025, 14:15:11) [MSC v.1944 64 bit (AMD64)]
PyTorch: 2.8.0+cu126
CUDA available: True
GPU: NVIDIA GeForce GTX 1660 Ti
GPU memory: 6.0 GB
============================================================

============================================================
TEST: Config loading with new parameters
============================================================
  Config OK: jpeg_augment=True, no_crop=0.3, test_like_val=True
  PASSED: Config loading with new parameters

============================================================
TEST: JPEGCompression augmentation
============================================================
  Input size: (62, 34), Output size: (62, 34)
  PASSED: JPEGCompression augmentation

============================================================
TEST: train_progressive.py config wiring
============================================================
  jpeg_augment occurrences: 4
  test_like_val_loader wired: 2 places
  All new params properly wired in train script
  PASSED: train_progressive.py config wiring

============================================================
TEST: Dataset creation with JPEG + no-crop + test-like-val
============================================================
  Train: 90 samples, 22 batches
  Val: 10 samples
  Test-like val: 5 samples
  Train batch: LR=torch.Size([4, 3, 34, 62]), HR=torch.Size([4, 3, 68, 124])
  Plate texts: ['ABC1234', 'JKL3456', 'GHI9012', 'JKL3456']
  Test-like batch: LR=torch.Size([4, 3, 34, 62]), HR=torch.Size([4, 3, 68, 124])
  PASSED: Dataset creation with JPEG + no-crop + test-like-val

============================================================
TEST: Dataset creation without test-like-val (3-tuple)
============================================================
  3-tuple returned correctly (no test_like_val)
  Train: 54 samples
  PASSED: Dataset creation without test-like-val (3-tuple)

============================================================
TEST: Generator forward pass
============================================================
  Generator parameters: 3,994,240
  Input: torch.Size([2, 3, 34, 62]) -> Output: torch.Size([2, 3, 68, 124])
  PASSED: Generator forward pass

============================================================
TEST: PARSeq OCR forward + predict
============================================================
Loading PARSeq model from torch.hub...
Using cache found in C:\Users\canht/.cache\torch\hub\baudm_parseq_main
C:\Python\Lib\site-packages\timm\models\helpers.py:7: FutureWarning: Importing from timm.models.helpers is deprecated, please import via timm.models
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.models", FutureWarning)
Successfully loaded PARSeq model
  PARSeq parameters: 23,832,671
  Native vocab size: 97 tokens
  Max label length: 25
  Training: teacher forcing + PLM (6 permutations)
  OCR parameters: 23,832,671
  Predictions: ['ALL', 'CALE']
  PASSED: PARSeq OCR forward + predict

============================================================
TEST: PARSeq forward_train loss computation
============================================================
Loading PARSeq model from torch.hub...
Using cache found in C:\Users\canht/.cache\torch\hub\baudm_parseq_main
Successfully loaded PARSeq model
  PARSeq parameters: 23,832,671
  Native vocab size: 97 tokens
  Max label length: 25
  Training: teacher forcing + PLM (6 permutations)
  forward_train loss: 3.8251
  Grad computed: True
  PASSED: PARSeq forward_train loss computation

============================================================
TEST: _simulate_test_pipeline (Stage 0 degradation)
============================================================
Loading PARSeq model from torch.hub...
Using cache found in C:\Users\canht/.cache\torch\hub\baudm_parseq_main
Successfully loaded PARSeq model
  PARSeq parameters: 23,832,671
  Native vocab size: 97 tokens
  Max label length: 25
  Training: teacher forcing + PLM (6 permutations)
Using cache found in C:\Users\canht/.cache\torch\hub\pytorch_vision_v0.13.0
C:\Python\Lib\site-packages\torchvision\models\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
C:\Python\Lib\site-packages\torchvision\models\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
  Input: hr=torch.Size([8, 3, 68, 124]), lr=torch.Size([8, 3, 34, 62])
  Result: torch.Size([8, 3, 68, 124]), unchanged: 4/8 (~4 expected)
  PASSED: _simulate_test_pipeline (Stage 0 degradation)

============================================================
TEST: _apply_ocr_augmentation
============================================================
Loading PARSeq model from torch.hub...
Using cache found in C:\Users\canht/.cache\torch\hub\baudm_parseq_main
Successfully loaded PARSeq model
  PARSeq parameters: 23,832,671
  Native vocab size: 97 tokens
  Max label length: 25
  Training: teacher forcing + PLM (6 permutations)
Using cache found in C:\Users\canht/.cache\torch\hub\pytorch_vision_v0.13.0
  Input: torch.Size([4, 3, 68, 124]) -> Augmented: torch.Size([4, 3, 68, 124])
  PASSED: _apply_ocr_augmentation

============================================================
TEST: Stage 0 full training step (synthetic)
============================================================
Loading PARSeq model from torch.hub...
Using cache found in C:\Users\canht/.cache\torch\hub\baudm_parseq_main
Successfully loaded PARSeq model
  PARSeq parameters: 23,832,671
  Native vocab size: 97 tokens
  Max label length: 25
  Training: teacher forcing + PLM (6 permutations)
Using cache found in C:\Users\canht/.cache\torch\hub\pytorch_vision_v0.13.0
  Stage 0 step OK: loss=3.9253
  Batch: LR=torch.Size([4, 3, 34, 62]), HR=torch.Size([4, 3, 68, 124])
  Texts: ['DEF5678', 'GHI9012', 'DEF5678', 'DEF5678']
  PASSED: Stage 0 full training step (synthetic)

============================================================
TEST: Generator+OCR pipeline (Stage 1+ flow)
============================================================
Loading PARSeq model from torch.hub...
Using cache found in C:\Users\canht/.cache\torch\hub\baudm_parseq_main
Successfully loaded PARSeq model
  PARSeq parameters: 23,832,671
  Native vocab size: 97 tokens
  Max label length: 25
  Training: teacher forcing + PLM (6 permutations)
  LR: torch.Size([4, 3, 34, 62]) -> SR: torch.Size([4, 3, 68, 124])
  Predictions: ['SK', 'COMMENI', 'MINUTA', 'ITATION']
  L1 loss: 0.8130, backward OK
  PASSED: Generator+OCR pipeline (Stage 1+ flow)

============================================================
TEST: Test-like validation loop
============================================================
Loading PARSeq model from torch.hub...
Using cache found in C:\Users\canht/.cache\torch\hub\baudm_parseq_main
Successfully loaded PARSeq model
  PARSeq parameters: 23,832,671
  Native vocab size: 97 tokens
  Max label length: 25
  Training: teacher forcing + PLM (6 permutations)
Using cache found in C:\Users\canht/.cache\torch\hub\pytorch_vision_v0.13.0
  Metrics: {'test_like_word_acc': 0.0, 'test_like_char_acc': 0.09523809523809523, 'test_like_psnr': 13.834281921386719}
  PASSED: Test-like validation loop

============================================================
TEST: LCOFL loss computation
============================================================
Using cache found in C:\Users\canht/.cache\torch\hub\pytorch_vision_v0.13.0
Loading PARSeq model from torch.hub...
Using cache found in C:\Users\canht/.cache\torch\hub\baudm_parseq_main
Successfully loaded PARSeq model
  PARSeq parameters: 23,832,671
  Native vocab size: 97 tokens
  Max label length: 25
  Training: teacher forcing + PLM (6 permutations)
  LCOFL loss: 2.4146
  LCOFL info keys: ['classification_loss', 'loss_mode', 'layout_penalty', 'layout_violations', 'total_chars', 'ssim_loss', 'total_loss']
G:\LP-ASRN\tests\smoke_test.py:633: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen/core/TensorBody.h:494.)
  print(f"  SR grad: {'exists' if sr.grad is not None else 'None (LCOFL uses logits path)'}")
  SR grad: None (LCOFL uses logits path)
  PASSED: LCOFL loss computation

============================================================
SMOKE TEST SUMMARY
============================================================
  Passed: 14/14
    + Config loading with new parameters
    + JPEGCompression augmentation
    + train_progressive.py config wiring
    + Dataset creation with JPEG + no-crop + test-like-val
    + Dataset creation without test-like-val (3-tuple)
    + Generator forward pass
    + PARSeq OCR forward + predict
    + PARSeq forward_train loss computation
    + _simulate_test_pipeline (Stage 0 degradation)
    + _apply_ocr_augmentation
    + Stage 0 full training step (synthetic)
    + Generator+OCR pipeline (Stage 1+ flow)
    + Test-like validation loop
    + LCOFL loss computation

  All tests passed! Pipeline is ready for deployment.
